{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import time, os\n",
    "import tensorflow as tf\n",
    "import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "STEPS_PER_CHECKPOINT = 5\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "TRAINING_DIR = './model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../dataset/mnist/mnist.pkl', 'rb') as f:\n",
    "    data = pickle.load(f, encoding='latin')\n",
    "\n",
    "\n",
    "(train_images, train_labels), (valid_images, valid_labels), (test_images, test_labels) = data\n",
    "#разбивка на тренировочные картинки, проверочные, тестовые.\n",
    "train_images = np.reshape(train_images, [-1, 28, 28, 1])\n",
    "train_labels = np.reshape(train_labels, [-1, 1])\n",
    "valid_images = np.reshape(valid_images, [-1, 28, 28, 1])\n",
    "valid_labels = np.reshape(valid_labels, [-1, 1])\n",
    "test_images = np.reshape(test_images, [-1, 28, 28, 1])\n",
    "test_labels = np.reshape(test_labels, [-1, 1])\n",
    "\n",
    "# дополнение до размера 32x32\n",
    "train_images = np.pad(train_images, ((0, 0), (2, 2), (2, 2), (0, 0)), mode='edge')\n",
    "valid_images = np.pad(valid_images, ((0, 0), (2, 2), (2, 2), (0, 0)), mode='edge')\n",
    "test_images = np.pad(test_images, ((0, 0), (2, 2), (2, 2), (0, 0)), mode='edge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#функция рисования результатов работы генератора\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.squeeze(), cmap='gray')\n",
    "\n",
    "    return fig\n",
    "\n",
    "def sample_seed_inputs(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GENERATOR_SEED_SIZE = 64\n",
    "\n",
    "def generator(inputs, batch_size, training):\n",
    "    with tf.name_scope('generator'):\n",
    "        net = layers.fully_connected_layer(1, inputs, 4 * 4 * 512, None)\n",
    "        net = tf.reshape(net, [batch_size, 4, 4, 512])\n",
    "        net = layers.batch_norm(net, training, name='bn1')\n",
    "        net = layers.conv2d_transpose_layer(1, net, [5, 5, 256], batch_size, stride=2)\n",
    "        net = layers.batch_norm(net, training, name='bn2')\n",
    "        net = layers.conv2d_transpose_layer(2, net, [5, 5, 128], batch_size, stride=2)\n",
    "        net = layers.batch_norm(net, training, name='bn3')\n",
    "        net = layers.conv2d_transpose_layer(3, net, [5, 5, 1], batch_size, tf.nn.sigmoid, stride=2, zero_biases=True)\n",
    "\n",
    "        return net\n",
    "\n",
    "\n",
    "def discriminator_base(inputs):\n",
    "    with tf.name_scope('discriminator_base'):\n",
    "        #net = layers.batch_norm(inputs, training, name='bn1')\n",
    "        net = layers.conv2d_layer(1, inputs, [5, 5, 16], lambda x: layers.lrelu(x, 0.2), stride=2)\n",
    "        #net = layers.batch_norm(net, training, name='bn2')\n",
    "        net = layers.conv2d_layer(2, net, [5, 5, 32], lambda x: layers.lrelu(x, 0.2), stride=2)\n",
    "        #net = layers.batch_norm(net, training, name='bn3')\n",
    "        net = layers.conv2d_layer(3, net, [5, 5, 64], lambda x: layers.lrelu(x, 0.2), stride=2)\n",
    "        #net = layers.batch_norm(net, training, name='bn4')\n",
    "        net = layers.conv2d_layer(4, net, [5, 5, 128], lambda x: layers.lrelu(x, 0.2), stride=2)\n",
    "        net = layers.max_pool2d(net, [2, 2])\n",
    "        #net = layers.batch_norm(net, training, name='bn5')\n",
    "\n",
    "        return net\n",
    "\n",
    "def discriminator(inputs):\n",
    "    with tf.name_scope('discriminator'):\n",
    "        net = layers.fully_connected_layer(1, inputs, 16)\n",
    "        net = layers.fully_connected_layer(2, net, 1, tf.nn.sigmoid, zero_biases=True, zero_weights=True)\n",
    "\n",
    "        return net\n",
    "\n",
    "def discriminator_latent(inputs, categorical_shape, continuous_shape):\n",
    "    with tf.name_scope('discriminator_latent'):\n",
    "        net = layers.fully_connected_layer(1, inputs, 16)\n",
    "\n",
    "        cat_net = layers.fully_connected_layer(2, net, categorical_shape, tf.nn.softmax, zero_biases=True, zero_weights=True)\n",
    "        con_net = layers.fully_connected_layer(3, net, continuous_shape, tf.nn.tanh, zero_biases=True, zero_weights=True)\n",
    "\n",
    "        return cat_net, con_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#обнуление графа\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#создание сети в графе\n",
    "with tf.name_scope('GAN'):\n",
    "    training_mode = tf.placeholder(tf.bool, name='training_mode')\n",
    "    categorical_inputs = tf.placeholder(tf.int32, [BATCH_SIZE, 1], name='categorical_inputs')\n",
    "    _categorical_inputs = tf.cast(tf.one_hot(tf.squeeze(categorical_inputs), 10), tf.float32)\n",
    "    _categorical_inputs = tf.reshape(_categorical_inputs, [BATCH_SIZE, 10])\n",
    "\n",
    "    continuous_inputs = tf.placeholder(tf.float32, [BATCH_SIZE, 2], name='continuous_inputs')\n",
    "\n",
    "    generator_seed_inputs = tf.placeholder(tf.float32, [BATCH_SIZE, GENERATOR_SEED_SIZE], name='generator_seed_inputs')\n",
    "\n",
    "    _generator_inputs = generator_seed_inputs\n",
    "    with tf.variable_scope('generator'):\n",
    "        _inputs = tf.concat([_generator_inputs, _categorical_inputs, continuous_inputs], axis=1)\n",
    "        generator_outputs = generator(_inputs, BATCH_SIZE, training_mode)\n",
    "\n",
    "    discriminator_inputs = tf.placeholder(tf.float32, [BATCH_SIZE] + list(train_images.shape[1:]), name='inputs')\n",
    "    with tf.variable_scope('discriminator') as vs:\n",
    "        with tf.name_scope('real'):\n",
    "            net = discriminator_base(discriminator_inputs)\n",
    "            discriminator_outputs_real_prob = discriminator(net)\n",
    "        vs.reuse_variables()\n",
    "        with tf.name_scope('fake'):\n",
    "            net = discriminator_base(generator_outputs)\n",
    "            discriminator_outputs_fake_prob = discriminator(net)\n",
    "\n",
    "        with tf.name_scope('latent'):\n",
    "            net = discriminator_base(generator_outputs)\n",
    "\n",
    "    with tf.variable_scope('discriminator-latent'):\n",
    "        latent_restored_outputs = discriminator_latent(net, _categorical_inputs.shape[1], continuous_inputs.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#элементы графа для обучения сети\n",
    "with tf.name_scope('training'):\n",
    "    cat_out, con_out = latent_restored_outputs\n",
    "\n",
    "    cat_loss = tf.reduce_mean(-tf.reduce_sum(_categorical_inputs*tf.log(tf.clip_by_value(cat_out, 1e-9, 1)), axis=1))\n",
    "    con_loss = tf.reduce_mean(0.5 * tf.square(continuous_inputs - con_out))\n",
    "\n",
    "    mutual_loss = cat_loss + con_loss\n",
    "\n",
    "    mutual_lambda = tf.Variable(1e-1, trainable=False)\n",
    "\n",
    "    with tf.name_scope('discriminator'):\n",
    "        discriminator_targets_real = tf.ones_like(discriminator_outputs_real_prob, name='discriminator_targets_real')\n",
    "        discriminator_targets_fake = tf.zeros_like(discriminator_outputs_fake_prob, name='discriminator_targets_fake')\n",
    "\n",
    "        _loss_real = tf.reduce_mean(tf.log(tf.clip_by_value(discriminator_outputs_real_prob, 1e-9, 1)))\n",
    "        _loss_fake = tf.reduce_mean(tf.log(tf.clip_by_value(1 - discriminator_outputs_fake_prob, 1e-9, 1)))\n",
    "        discriminator_loss = _loss_real + _loss_fake - mutual_lambda * mutual_loss\n",
    "\n",
    "        #минимизация функции потерь по весовым коэффициентам\n",
    "        discriminator_lr_var = tf.Variable(1e-3, trainable=False)\n",
    "\n",
    "        params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
    "        optimizer = tf.train.AdamOptimizer(discriminator_lr_var)\n",
    "        discriminator_updates = optimizer.minimize(-discriminator_loss, var_list=params) # maximization\n",
    "\n",
    "    with tf.name_scope('generator'):\n",
    "        #целевые значения, к которым должна придти сеть в результате обучения\n",
    "        generator_targets = tf.ones_like(discriminator_outputs_fake_prob, name='generator_targets')\n",
    "        #функция потерь (ошибки)\n",
    "        generator_loss = tf.reduce_mean(tf.log(tf.clip_by_value(discriminator_outputs_fake_prob, 1e-9, 1))) + mutual_lambda * mutual_loss\n",
    "\n",
    "        #минимизация функции потерь по весовым коэффициентам\n",
    "        generator_lr_var = tf.Variable(1e-3, trainable=False)\n",
    "\n",
    "        params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "        optimizer = tf.train.AdamOptimizer(generator_lr_var)\n",
    "        generator_updates = optimizer.minimize(-generator_loss, var_list=params) # maximization\n",
    "\n",
    "# сохранение параметров для графа\n",
    "save_vars = tf.global_variables()\n",
    "saver = tf.train.Saver(save_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# шаг тренировки. заполняем узлы графа, картинками для обучения, которые используются для входа, и вычисляем обновления весов\n",
    "def train_discriminator_step(session, images, categorical, continuous, seed):\n",
    "    input_feed = {}\n",
    "\n",
    "    input_feed[discriminator_inputs.name] = images\n",
    "    input_feed[categorical_inputs.name] = categorical\n",
    "    input_feed[continuous_inputs.name] = continuous\n",
    "    input_feed[generator_seed_inputs.name] = seed\n",
    "    input_feed[training_mode.name] = True\n",
    "\n",
    "    output_feed = [discriminator_updates]\n",
    "\n",
    "    _ = session.run(output_feed, input_feed)\n",
    "\n",
    "def train_generator_step(session, categorical, continuous, seed):\n",
    "    input_feed = {}\n",
    "\n",
    "    input_feed[categorical_inputs.name] = categorical\n",
    "    input_feed[continuous_inputs.name] = continuous\n",
    "    input_feed[generator_seed_inputs.name] = seed\n",
    "    input_feed[training_mode.name] = True\n",
    "\n",
    "    output_feed = [generator_updates]\n",
    "\n",
    "    _ = session.run(output_feed, input_feed)\n",
    "\n",
    "def generator_step(session, categorical, continuous, seed):\n",
    "    input_feed = {}\n",
    "\n",
    "    input_feed[categorical_inputs.name] = categorical\n",
    "    input_feed[continuous_inputs.name] = continuous\n",
    "    input_feed[generator_seed_inputs.name] = seed\n",
    "    input_feed[training_mode.name] = False\n",
    "\n",
    "    return session.run(generator_outputs, input_feed)\n",
    "\n",
    "# заполнение узлов графа картинками и целевыми значениями.расчет функции потерь\n",
    "def valid_step(session, images, categorical, continuous, seed, summary):\n",
    "    input_feed = {}\n",
    "\n",
    "    input_feed[categorical_inputs.name] = categorical\n",
    "    input_feed[discriminator_inputs.name] = images\n",
    "    input_feed[continuous_inputs.name] = continuous\n",
    "    input_feed[generator_seed_inputs.name] = seed\n",
    "    input_feed[training_mode.name] = False\n",
    "\n",
    "    output_feed = [generator_loss, discriminator_loss, summary]\n",
    "\n",
    "    return session.run(output_feed, input_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# цикл обучения\n",
    "\n",
    "if not os.path.exists(TRAINING_DIR):\n",
    "    os.makedirs(TRAINING_DIR)\n",
    "\n",
    "if not os.path.exists('./output/'):\n",
    "    os.makedirs('./output/')\n",
    "\n",
    "checkpoint_path = os.path.join(TRAINING_DIR, 'GAN.ckpt')\n",
    "\n",
    "tf.summary.scalar('geneartor loss', generator_loss)\n",
    "tf.summary.scalar('generator learning rate', generator_lr_var)\n",
    "\n",
    "tf.summary.scalar('discriminator loss', discriminator_loss)\n",
    "tf.summary.scalar('discriminator learning rate', discriminator_lr_var)\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "nbatches = len(train_images) // BATCH_SIZE\n",
    "\n",
    "with tf.Session() as session:\n",
    "    train_summary_writer = tf.summary.FileWriter(os.path.join(TRAINING_DIR, 'summary', 'train'), session.graph)\n",
    "    valid_summary_writer = tf.summary.FileWriter(os.path.join(TRAINING_DIR, 'summary', 'valid'), session.graph)\n",
    "\n",
    "    print('Initializing parameters ', flush=True, end='')\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    print('[OK]', flush=True)\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(TRAINING_DIR)\n",
    "    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "        saver.restore(session, ckpt.model_checkpoint_path)\n",
    "\n",
    "    tf.train.write_graph(session.graph_def, TRAINING_DIR, 'GAN.pb', as_text=False)\n",
    "\n",
    "    print('Start training.', flush=True)\n",
    "    try:\n",
    "        for epoch in range(0, EPOCHS):\n",
    "            cat = np.random.randint(0, 10, [BATCH_SIZE, 1])\n",
    "            con = np.random.uniform(-1, 1, size=[BATCH_SIZE, 2])\n",
    "            seed = sample_seed_inputs(BATCH_SIZE, GENERATOR_SEED_SIZE)\n",
    "            samples = generator_step(session, cat, con, seed)[:16]\n",
    "            fig = plot(samples)\n",
    "            plt.savefig('output/{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            print('Epoch #%i: ' % (epoch+1), end='', flush=True)\n",
    "\n",
    "            for b in range(nbatches):\n",
    "                batch = np.arange(b*BATCH_SIZE, (b+1)*BATCH_SIZE)\n",
    "\n",
    "                images = train_images[batch]\n",
    "                cat = np.random.randint(0, 10, [BATCH_SIZE, 1])\n",
    "                con = np.random.uniform(-1, 1, size=[BATCH_SIZE, 2])\n",
    "                seed = sample_seed_inputs(BATCH_SIZE, GENERATOR_SEED_SIZE)\n",
    "\n",
    "                train_discriminator_step(session, images, cat, con, seed)\n",
    "                train_generator_step(session, cat, con, seed)\n",
    "\n",
    "            batch = np.random.choice(len(train_images), BATCH_SIZE, replace=False)\n",
    "            images = train_images[batch]\n",
    "            cat = np.random.randint(0, 10, [BATCH_SIZE, 1])\n",
    "            con = np.random.uniform(-1, 1, size=[BATCH_SIZE, 2])\n",
    "            seed = sample_seed_inputs(BATCH_SIZE, GENERATOR_SEED_SIZE)\n",
    "            train_gen_loss, train_dis_loss, summary = valid_step(session, images, cat, con, seed, summary_op)\n",
    "\n",
    "            train_summary_writer.add_summary(summary, epoch)\n",
    "\n",
    "            batch = np.random.choice(len(valid_images), BATCH_SIZE, replace=False)\n",
    "            images = test_images[batch]\n",
    "            cat = np.random.randint(0, 10, [BATCH_SIZE, 1])\n",
    "            con = np.random.uniform(-1, 1, size=[BATCH_SIZE, 2])\n",
    "            seed = sample_seed_inputs(BATCH_SIZE, GENERATOR_SEED_SIZE)\n",
    "            valid_gen_loss, valid_dis_loss, summary = valid_step(session, images, cat, con, seed, summary_op)\n",
    "            valid_summary_writer.add_summary(summary, epoch)\n",
    "\n",
    "            elapsed = time.time() - start\n",
    "            print('train generator loss = %.6f, train discriminator loss = %.6f, valid generator loss = %.6f, valid discriminator loss = %.6f, elapsed %.3f sec.' % (train_gen_loss, train_dis_loss, valid_gen_loss, valid_dis_loss, elapsed), flush=True)\n",
    "\n",
    "            if (epoch+1) % STEPS_PER_CHECKPOINT == 0:\n",
    "                saver.save(session, checkpoint_path)\n",
    "\n",
    "        print('Training process is finished.', flush=True)\n",
    "\n",
    "        cat = np.random.randint(0, 9, [BATCH_SIZE, 1])\n",
    "        con = np.random.uniform(-1, 1, size=[BATCH_SIZE, 2])\n",
    "        seed = sample_seed_inputs(BATCH_SIZE, GENERATOR_SEED_SIZE)\n",
    "        samples = generator_step(session, cat, con, seed)[:16]\n",
    "        fig = plot(samples)\n",
    "        plt.savefig('output/{}.png'.format(str(EPOCHS).zfill(3)), bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "    finally:\n",
    "        saver.save(session, checkpoint_path)\n",
    "        tf.train.write_graph(session.graph_def, TRAINING_DIR, 'GAN.pb', as_text=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
